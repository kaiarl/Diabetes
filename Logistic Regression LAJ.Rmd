---
title: "Diabetes Logistic Regression"
author: "Lee Ann Johnson"
date: "12/12/2021"
output: 
  pdf_document:
    latex_engine: xelatex

---

# Set up
```{r include = FALSE}

knitr::opts_chunk$set(include = FALSE, results = 'hide', message = FALSE)

# Import libraries
library(tidyverse)
library(ROCR)
library(GGally)
library(MASS)
library(faraway)
```

# Executive Summary

In the United States, the number of people who are diagnosed with diabetes has been steadily increasing. This is true for both children and adults. Diabetes is considered a chronic condition, meaning that it is long lasting, typically for the rest of an individual’s life. Those with diabetes have difficulty using the food they eat for energy in the body.  When a person eats food, the body breaks down the sugars in the food, also known as glucose, so that it can enter the bloodstream. Glucose is energy for the body. As the levels of glucose in the bloodstream rise, the body produces a substance known as insulin. Insulin allows the glucose in the blood to be used by cells in the body. Individuals who have been diagnosed with diabetes either cannot use the insulin they make (Type 2 Diabetes) or do not make enough insulin (Type 1 Diabetes).

Some groups of people have a higher risk of developing diabetes. For example, when compared to the general population, some groups, such as women of American Indian/Alaskan Native heritage, are more likely to be diagnosed with the disease. Other important risk factors for diabetes have been identified. Those with a high body mass index (BMI), high blood pressure, or who are 65 or older have a higher likelihood of developing diabetes.

It is important to understand diabetes in populations that are at high risk for developing the disease. A study at the University of Virginia School of Data Science sought to do just that. The study focused on women of Pima Indian heritage and tried to predict which of the women would be diagnosed with diabetes as well as levels of glucose in the blood. The researchers identified several aspects of health that might be related to a diabetes diagnosis or the amount of glucose in the bloodstream; number of pregnancies, glucose levels, diastolic blood pressure, BMI, family history of diabetes, and age. 

The researchers found approximately 36% of women in the study, who were between 21 years old and 70 years old, were diagnosed with diabetes. They also discovered that when compared to women without diabetes, the women with diabetes tended to have had more pregnancies and a higher level of glucose in their bloodstream. The women also seemed to have a higher BMI and to be older. Findings in the study identified most women in the sample, regardless of a diabetes diagnosis, reported a BMI over 25. A BMI over 25 places individuals at an increased risk of diabetes.

In women of Pima Indian heritage, researchers found four factors that were statistically important in predicting a diabetes diagnosis ; number of pregnancies, level of glucose in the blood, BMI, and family history of diabetes. For women of Pima Indian heritage, as pregnancies, glucose in the bloodstream, BMI and families members with diabetes increased, so did the chances of developing diabetes. The other factors investigated by the study (blood pressure and age) were not statistically important when trying to predict a diagnosis of diabetes. 

Researchers also tried to create a model that could better predict an individual’s level of glucose in the bloodstream. The investigative team found that a person’s blood pressure, BMI, age, and their family history of diabetes could predict the level of glucose in the blood. Unfortunately, these four factors explained very little, only 13%, of the changes in glucose. Because of this, researchers recommended that future studies investigate other factors that may help healthcare providers understand levels of glucose in the bloodstream. 

Healthcare providers can now better assess the risk of diabetes in women of Pima Indian heritage. This can help providers identify which women are most at risk of developing the disease. Once providers are aware of who is at high risk, better recommendations for diabetes screening can be made. Similar models can be made to help healthcare providers to assess the risk of diabetes in other groups of people. These models can improve healthcare in underserved areas, where resources for diabetes screening may be scarce. 

```{r}
# Load data
Data <- read.csv("diabetes2.csv", header=T)
print(head(Data))
```

The data has 768 rows and 9 columns. One of these columns (Outcome) will be our response variable for our logistic regression and the others are potential predictors.

```{r}
# Check dimensions of data
print(dim(Data)) # 768 rows and 9 columns
```

```{r}
# Display names of all columns
print(colnames(Data))
```

Before we go any further with our analysis we will split our data into a training and test set. We've chosen a random seed of "123" for reproducability. We will do all further analysis, visualization, and model building using our training data and then use our test data to evaluate our model's performance on unseen data. Since our data isn't particularly large (768 rows), we chose a 60/40 split so that we would have more data for training and thus have more stable model results.
```{r}
# Randomly split data into two halves
set.seed(123) # For reproducability
sample<-sample.int(nrow(Data), floor(.60*nrow(Data)), replace = F)
train<-Data[sample, ] # Training data
test<-Data[-sample, ] # Test data
```


# Data Cleaning
Before creating our linear models, we needed to make sure that our data was clean. First, we checked for missing values. At first glance there did not appear to be any missing values. 
```{r}
# Check for missing values in each column
colSums(is.na(train))
```

Next, we created a summary of each variable in our data including the mean and five number summary. 

```{r}
# Summary of columns
print(summary(train))
```

This summary shows us the distribution of each variable. For example, we can see that the patients in the dataset range from age 21 to 70 with a median of 29 and mean of 33.3. Since we mean age is greater than the median age we can tell that the age distribution is skewed right. From this summary of the data we also observe some nonsensical values. For example some participants have a recorded BMI of 0 or an insulin value of 0, which cannot occur logically. Although there were no missing values above, we suspect that 0 was zero was recorded in place of missing data. The only variables for which a zero makes sense are pregnancies and outcome. For any other variable we'll fill zero with unknown and then evaluate the best way to handle unknowns. We will do this for both the train and test sets even though we won't be using the test data until later.

```{r}
# List of columns where zero is non-sensical (i.e. zero indicates unknown)
# All predictor columns other than pregnancies
zero_unknown_cols <-c("Glucose", "BloodPressure", "SkinThickness",
                      "Insulin", "BMI", "DiabetesPedigreeFunction",
                      "Age")

# Replace zeros with unknowns
train[,zero_unknown_cols] <- replace(train[,zero_unknown_cols], train[,zero_unknown_cols]==0,NA)
test[,zero_unknown_cols] <- replace(test[,zero_unknown_cols], test[,zero_unknown_cols]==0,NA)
```

Now that we have encoded our unknown values we checked what percent of observations were unknown for each variable. 

```{r}
# Check for missing values in each column again
round(colSums(is.na(train))/dim(train)[1],2)
```

We observed that a large portion of some columns have missing data, especially Insulin with 48% missing and SkinThickness with 30%. Now that we've identified the missing values we needed to handle them before we continue with our analysis. We removed variables with over 25% missing (Insulin and Skin Thickness) because we didn't think they would be reliable predictors with so much missing data and we didn't want to drop nearly 50% of our data.

```{r}
# Remove variables with high percent missing
train <- select(train, -c('SkinThickness', 'Insulin'))
test <- select(test, -c('SkinThickness', 'Insulin'))
```

For the remaining data we chose to remove any unknowns. Before doing so we checked what percent of the data would be dropped. Seeing that this was only about 6% we thought that was reasonable given that it would give us a complete dataset so we went ahead and dropped any rows with missing values.
```{r}
# Check what percent of our test data we will drop
round(sum(!complete.cases(train))/dim(train)[1],2)
```

```{r}
# Drop rows with missing data from our train and test data
train <- train[complete.cases(train), ]
test <- test[complete.cases(test), ]
```

Once the data was cleaned, we examined the basic descriptive statistics of our data again. There were only minor changes in the median and means of the data, with the biggest changes in the range of values. Columns no longer had 0 as a minimum value. The age ranged from 21 years old to 70 years old, plasma glucose levels ranged from 56 to 199, blood pressure ranged from 24 to 80, and BMI ranged from 18.40 to 67.10.

```{r}
# Summary of columns
print(summary(train))
```


Our outcome variable (Diabetes) was originally labeled as 0/1. We converted this to a factor and labeled the outcomes so that R treated it as a categorical response.
```{r}
# Create diabetes factor column with labels from outcome column 
train$Outcome<-factor(train$Outcome)
levels(train$Outcome) <- c("No", "Yes")

# For the test data too
test$Outcome<-factor(test$Outcome)
levels(test$Outcome) <- c("No", "Yes")
```


# Analysis and Visualization

## Distribution of Reponse Variable
```{r}
# Calculate proportion of patients with diabetes
outcome_prop <- round(prop.table(table(train$Outcome)),2)
outcome_prop
# Bar plot of distribution
ggplot(train, aes(x=Outcome)) +
  geom_bar(fill="blue") +
  labs(x="Diabetes Outcome", y="Frequency", title="Distribution of Diabetes Outcome")

```

About 36% of patients in this data set have diabetes while 64% do not. More don't have diabetes, but this is not a huge imbalance.

## Visualizaling Response and Potential Predictors
### Pregnancies
```{r}
# Side by side box plot
ggplot(train, aes(x=Outcome, y=Pregnancies))+
  geom_boxplot(color = "blue", outlier.color = "orange") +
  labs(x="Diabetes", y="Number of Pregnancies", title="Distribution of Pregnancies by Diabetes Outcome")
```
There is a fair amount of overlap in these box plots, but they do suggest that patients with diabetes tend to have higher number of pregnancies. The range of values is wider in women diagnosed with diabetes. We also observe this in the density plots where patients without diabetes are more likely to have had small numbers of or no pregnancies, with most of these women reporting 2 pregnancies. 

```{r}
# Density plot
ggplot(train, aes(x=Pregnancies, color=Outcome)) +
  geom_density() +
  labs(x="Number of Pregnancies", y="Density", title="Density Plot of Pregnancies by Diabetes Outcome")
```

### Glucose
```{r}
# Side by side box plot
ggplot(train, aes(x=Outcome, y=Glucose))+
  geom_boxplot(color = "blue", outlier.color = "orange") +
  labs(x="Diabetes", y="Glucose Concentration", title="Distribution of Glucose Concentration by Diabetes Outcome")
```

These box plots (especially from Q1 to Q3) have little overlap so we suspect that glucose will be predictive in identifying which patients have diabetes. From this visual we can see that patients with diabetes tend to have higher plasma glucose concentrations after a 2 hour oral glucose tolerance test. Values of over 200 indicate diabetes while values between 140 and 199 indicate prediabetes. The boxplot shows that most women diagnosed with diabetes have plasma glucose levels in the prediabetes range while those who do not have plasma glucose levels within the normal range. 

### Blood Pressure

```{r}
# Density plot
ggplot(train, aes(x=BloodPressure, color=Outcome)) +
  geom_density() +
  labs(x="Blood Pressure", y="Density", title="Density Plot of Blood Pressure by Diabetes Outcome")
```

There is quite a bit of overlap in the density plots of diastolic blood pressure for those that do and do not have diabetes. While there is a lot of overlap we do observe that those with diabetes tend to have slightly higher diastolic blood pressure than those that do not. Normal diastolic blood pressure is less 80 mm Hg and those with a diastolic blood pressure above 90 mg Hg are considered to have high blood pressure. Prehypertension range from 80 mm Hg to 89 mm Hg. 

### BMI

```{r}
# Side by side box plot
ggplot(train, aes(x=Outcome, y=BMI))+
  geom_boxplot(color = "blue", outlier.color = "orange") +
  labs(x="Diabetes", y="BMI", title="Distribution of BMI by Diabetes Outcome")
```

Patients with diabetes tend to have higher BMIs than those without. Those with a BMI over 25 are at higher risk for diabetes. In this sample, the box plot shows the BMI range for those with diabetes begins around 25.  

### Diabetes Pedigree Function

```{r}
# Density plot
ggplot(train, aes(x=DiabetesPedigreeFunction, color=Outcome)) +
  geom_density() +
  labs(x="Diabetes Pedigree Function", y="Density", title="Density Plot of Diabetes Pedigree Function by Diabetes Outcome")
```

Those with lower Diabetes Pedigree Functions (less family history of diabetes) appear less likely to have diabetes. 

### Age

```{r}
# Side by side box plot
ggplot(train, aes(x=Outcome, y=Age))+
  geom_boxplot(color = "blue", outlier.color = "orange") +
  labs(x="Diabetes", y="Age", title="Distribution of Age by Diabetes Outcome")
```

The median age of patients with diabetes is higher than those that do not have diabetes. The risk for developing diabetes begins to increase around age 45 and continues to increase as people age. The box plot shows the median age for both groups as under 45. However, we know those from minority populations, such as in this sample, are often diagnosed with diabetes at an earlier age. 


### Multiple Predictors and Response

Next, we created scatter plots between pairs of potential predictors and colored the dots with the diabetes outcome. From this scatter plot of BMI versus age there does seem to be a relationship with diabetes where the women with diabetes tend to be higher BMIs and older as well. 

```{r}
# Scatter plot of potential predictors (colored by response)
# bmi and blood pressure colored by diabetes
ggplot(train, aes(x=BMI, y=Age, color=Outcome)) +
  geom_point() + 
  labs(x="BMI", y="Age", title="Scatterplot of BMI versus Age by Diabetes")

```

# Logistic Regression Model

Now that we had cleaned and visualized our data we could start fitting our models. For logistic regression we wanted to predict which patients have diabetes. 
## Fit Initial Model
For our initial logistic regression model we included all of our potential predictors (after dropping those with higher percent missing values), which included Pregnancies, Glucose, Blood Pressure, BMI, Diabetes Pedigree Function, and Age. 
```{r}
# Fit initial regression model
full <- glm(Outcome~., family="binomial", data=train)
summary(full)
```

Based on the summary of this initial logistic regression model we observe that Glucose and BMI and highly significant in predicting who has diabetes. These two variables had the largest differences in the density and box plots above so it is not surprising that they are highly predictive of diabetes. Pregnancies is also significant at a 5% significance level. The other predictors, Blood Pressure, Diabetes Pedigree Function, and Age do not add as much value given all of the other predictors are fit in the model. This does not mean that these three variables are not related to diabetes, it only means that they may not add value given that the other variables are also included in the model. 


## Test Hypothesis on Subset of Parameters

We will test whether we can drop all three of these insignificant predictors. For this test our null hypothesis will be $H_0: \beta_{BloodPressure} = \beta_{DiabetesPredigreeFunction} = \beta_{Age} = 0$ and the alternative is $H_A:$ at least one $\beta$ in $H_0$ is non-zero. 

```{r}
reduced <- glm(Outcome~Pregnancies + Glucose + BMI, family="binomial", data=train)
summary(reduced)
```

All of the predictors in this three variable model are statistically significant. Let's test whether we can use this reduced model by calculating the difference in deviance between this reduced model and our full model. 

```{r}
# Test statistic
test_stat <- reduced$deviance - full$deviance
# P-value
p_value <- 1 - pchisq(test_stat, 3)
p_value
```

This p-value of 0.048 is less than our significance level of 0.05 so we reject the null hypothesis and conclude that at least one of the predictors that we dropped is useful in predicting diabetes. Looking back at the standard errors of our full model, we observed that Age and Diabetes Pedigree Function had p-values of 0.06, just above our cutoff of 0.05 so perhaps these two variables should remain in our model.

```{r}
reduced2 <- glm(Outcome~Pregnancies + Glucose + BMI + DiabetesPedigreeFunction + Age, family="binomial", data=train)
summary(reduced2)
```

In this 5 variable model we observe that neither Age nor Diabetes Pedigree Function are statistically significant. Perhaps we should keep only Diabetes Pedigree Function because Age has a higher p-value. 

```{r}
reduced3 <- glm(Outcome~Pregnancies + Glucose + BMI + DiabetesPedigreeFunction, family="binomial", data=train)
summary(reduced3)
```

Alas, in this four variable model all of the predictors are statistically significant. We will confirm this result by testing whether we can drop Age and Blood Pressure from our full model. For this test our null hypothesis will be $H_0: \beta_{BloodPressure} = \beta_{Age} = 0$ and the alternative is $H_A:$ at least one $\beta$ in $H_0$ is non-zero. 

```{r}
# Test statistic
test_stat2 <- reduced3$deviance - full$deviance
# P-value
p_value2 <- 1 - pchisq(test_stat2, 2)
p_value2
```

The p-value of 0.16 is not less than our significance level of 0.05 so we fail to reject the null hypothesis. We don't have enough evidence to say that any of the two variables (Age and Blood Pressure) we dropped in our new reduced model have a non-zero coefficient and thus we can use our reduced model with just Pregnancies, Glucose, BMI, and Diabetes Pedigree Function to predict diabetes.

## Test Whether Model is Useful
Next, we'll test whether our reduced model is useful in predicting diabetes. In other words, can we drop all coefficients from our model? We'll start from our reduced model with four predictors given the results from our hypothesis test above. For this test our null hypothesis will be $H_0: \beta_{Pregnancies} = \beta_{Glucose} = \beta_{BMI} = \beta_{DiabetesPedigreeFunction} = 0$ and the alternative is $H_A:$ at least one $\beta$ in $H_0$ is non-zero. 

```{r}
# Test statistic
test_stat3 <- reduced3$null.deviance - reduced$deviance
# P-value
p_value3 <- 1 - pchisq(test_stat3, 4)
p_value3
```

The p-value for this hypothesis is 0 so we reject the null hypothesis and conclude that at least one of predictors has a non-zero coefficient and thus this logistic regression model is useful in estimating the odds of developing diabetes.

## Interpreting Model's Coefficients

```{r}
reduced3
```

Our estimated logistic regression equation is: $log(\dfrac{\hat{\pi}}{1-\hat{\pi}}) = -8.90583 + 0.13145*Pregnancies + 0.03582*Glucose + 0.08697*BMI + 0.73497*DiabetesPedigreeFunction$. All three of these predictors have positive coefficients, suggesting that for larger number of pregnancies, higher glucose concentration, higher BMI, and higher diabetes pedigree function the odds of having diabetes goes up, with all other variables held constant.

The estimated coefficient for pregnancies is 0.13145. This means that for each additional pregnancy the log odds that the woman will have diabetes increases by 0.13145 while controlling for glucose concentration, BMI, and diabetes pedigree function. In other words, for each one unit increase in number of pregnancies the odds that the woman has diabetes is multiplied by 1.140481, while holding the other variables constant. The estimated coefficient for glucose is 0.03582. This means that for each one unit increase in plasma glucose concentration the log odds that the woman will have diabetes increases by 0.03582, while holding the other variables constant. The estimated coefficient for BMI is 0.08697, meaning that for each one unit increase in BMI, the log odds that the patient has diabetes increases by 0.08697 while controlling for glucose concentration, pregnancies, and diabetes pedigree function. Finally, the estimated coefficient for diabetes pedigree function is 0.73497, suggesting that for each one unit increase in diabetes pedigree function the log odds that the patient has diabetes increases by 0.73497 while holding the other variables constant. 


## Model Evaluation

Use model built on training data to estimate the probabilities for observations in the unseen test data set. 

### Plot ROC curve

```{r}
# Predicted diabetes rate
preds <- predict(reduced3, newdata=test, type="response")
# Transform input data
rates <- prediction(preds, test$Outcome)
# Store true positive and false positive rates
roc_result <- performance(rates, measure="tpr", x.measure="fpr")
# Plot roc curve and overly diagonal (random)
plot(roc_result, main = "ROC curve for Diabetes")
lines(x=c(0,1), y=c(0,1), col="red")
```

The ROC curve lies above the straight diagonal line, suggesting that this model identifies/classifies people who have diabetes better than random.  

### Calculate AUC

```{r}
auc <- performance(rates, measure="auc")
auc@y.values
```

The value of AUC for the ROC curve above is 0.8386144. Since this value is greater than 0.5 the model does better than random for classifying who develops diabetes. 

### Create Confusion Matrix

```{r}
threshold = 0.5 # Define threshold
table(test$Outcome, preds>threshold) 
```

```{r}
# Accuracy
(174+52)/(174+22+43+52)
# TPR
52/(43+52)
# TNR
174/(174+22)
```

At a threshold of 0.5, the models' overall accuracy is 78% with a true positive rate of 55% and a true negative rate of 89%. These evaluation metrics suggest that the model is successful in identifying patients that develop diabetes. 


# Multiple Linear Regression

```{r}
# Raw diabetes set (768 rows by 9 variables)
diabetes <- as_tibble(read.csv('diabetes2.csv'))


# Change Outcome variable to factor with appropriate labels
diabetes$Outcome <- factor(diabetes$Outcome)
levels(diabetes$Outcome)
levels(diabetes$Outcome) <- c("No","Yes")
levels(diabetes$Outcome)

# Diabetes set without Insulin, Skin Thickness and complete cases only (724 rows by 7 variables)
diabetes2 <- diabetes

zero_unknown_cols <-c("Glucose", "BloodPressure", "SkinThickness",
                      "Insulin", "BMI", "DiabetesPedigreeFunction",
                      "Age")

diabetes2[,zero_unknown_cols] <- replace(diabetes2[,zero_unknown_cols], diabetes2[,zero_unknown_cols]==0,NA)

diabetes2 <- dplyr::select(diabetes2, -c('SkinThickness', 'Insulin'))
diabetes2 <- diabetes2[complete.cases(diabetes2),]
```

## Scatterplot

```{r}
GGally::ggpairs(diabetes2)

# All of the potential explanatory variables have positive correlation to glucose, though none have a correlation coefficient above 27%.
```

## Full MLR with Pregnancies, Blood Pressure, BMI, DiabetesPedigeeFunction, and Age

```{r}
result <- lm(Glucose ~ Pregnancies + BloodPressure + BMI + DiabetesPedigreeFunction + Age, data = diabetes2)
summary(result)

# The full model is highly statistically significant in predicting glucose, judging by the F-test statstic.  
# However, one of its explanatory variables, Pregnencies, is not (t-test p-value of 71%). 
```


```{r}
yhat <- result$fitted.values
res <- result$residuals
diabetes2 <- data.frame(diabetes2, yhat, res)

ggplot(diabetes2, aes(x = yhat,y = res))+
  geom_point()+
  geom_hline(yintercept=0, color="red")+
  labs(x="Fitted y",
       y="Residuals",
       title="Residual Plot from full MLR")
       
# The residual scatterplot shows non-constant variance, suggesting a transformation on the y-variable is needed. 
```


```{r}
boxcox(result)

# According to the Box-Cox plot, we should take the log of the y-variable (glucose). 
```


```{r}
acf(result$residuals)

# The ACF plot does not show any significant autocorrelation. 
```


```{r}
qqnorm(result$residuals)
qqline(result$residuals, col = "red")

# The QQ plot is not a perfect line, but it is close enough. 
```


```{r}
ystar <- log(diabetes2$Glucose)
diabetes2 <- data.frame(diabetes2, ystar)
```


## Final MLR with Blood Pressure, BMI, DiabetesPedigeeFunction, and Age
```{r}
resultLog <- lm(ystar ~ BloodPressure + BMI + DiabetesPedigreeFunction + Age, data = diabetes2)
summary(resultLog)

# Here we fit a log-transformed version of the final model (without Pregnancies). 
# The model is highly significant (p-value of F-test is nearly 0). 
# All of the individual explanatory variables are significant at the 95% confidence level. 
# Unfortunately, the adjusted R-squared of the model is only 12.24%, which means it only explains 12.24% of the variation in blood glucose. 
```


```{r}
yhatLog <- resultLog$fitted.values
resLog <- resultLog$residuals
diabetes2 <- data.frame(diabetes2, yhatLog, resLog)

ggplot(diabetes2, aes(x = yhatLog,y = resLog))+
  geom_point()+
  geom_hline(yintercept=0, color="red")+
  labs(x="Fitted log y",
       y="Residuals",
       title="Residual Plot from Reduced 3 log")
       
# The residual plot looks much better (mostly random). 
```


```{r}
boxcox(resultLog)

# The Box-Cox plot lines include 1, so no further transformation is necessary. 
```


```{r}
acf(resultLog$residuals)

# The ACF plot does not show any significant autocorrelation. 
```



```{r}
qqnorm(resultLog$residuals)
qqline(resultLog$residuals, col = "red")

# The QQ plot is not a perfect line, but it is close enough. 
```



```{r}
vif(resultLog)

# The VIFs are very low and suggest that multicollinearity is not a problem. 
```

Our data analysis establishes two main findings for our sample of women. First, it is possible to
predict blood glucose as a function of blood pressure, BMI, family history, and age. This requires a log transformation in order to meet the assumptions of linear regression, but it produces a model that is highly statistically significant and in which every explanatory variable is significant as well. Unfortunately, the model only explains about 13% of the variation in blood glucose. If the R2 were higher, there would be numerous potential clinical applications of this model. Without drawing blood, healthcare providers could estimate the blood glucose of an individual woman or group of women, helping to narrow the population of individuals formally tested for abnormal blood glucose. Future research could uncover any other simple explanatory variables that increase the predictive power of the model. Other researchers could use these findings as the foundation of a more robust model that can predict blood glucose for men as well. 

Our second major finding is that a combination of prior pregnancies, blood glucose, BMI, and
family history can predict the risk of developing diabetes in women considerably better than a random guess. Once again, there are potential clinical as well as research implications. Having a simple model to assess diabetes risk could be very useful to healthcare providers, especially in areas of the world where formal diagnostic facilities are rare. Such a model may assist in automated screening of patients, flagging those who are at the highest risk. Aside from extending the model to men (and presumably dropping the pregnancy variable), future researchers could investigate possible ways to use the first model to predict blood glucose, then using the second model to assess diabetes risk. This would improve accuracy of the screening process by identifying individuals who are most likely to have high plasma glucose levels and those who have the highest risk for diabetes. This is especially beneficial in medically under-served areas where healthcare providers and testing resources are scarce.

Our final model is statistically significant as a whole, and all of its explanatory variables are significant as well. The final model satisfies all the assumptions of linear regression. Multicollinearity is not a problem. The only issue is the low adjusted R-squared, which reduces the practical usefulness of the model.

